<section id="about">
  <h1>About Me</h1>
  <p class="tagline">M.S. AI @ NCKU MMCV Lab | Research Student</p>
  <h2>Hello there!</h2>
  I'm Liu Shih-Wen (Casper),
  <!-- <p>I'm 成功大學多媒體與電腦視覺實驗室 (Casper), Taiwan. My journey began teaching Scratch to neighborhood kids and guiding my little sibling from block coding to Python—sparked by that hands-on, project-first approach, I fell in love with tech and teaching. Today I’m pursuing my master’s in AI at NCKU’s Multimedia & Computer Vision Lab, where I’ve built PathGenIC (automatic histopathology report generation), squeezed 12× speed-ups out of glomerulus detectors with MedViT ensembles, and navigated massive MIMIC-III EHR datasets.</p> -->
  <p>In the lab I thrive on parameter-efficient fine-tuning (LoRA modules for ViTs and Swin, QA-ViT & MTLoRA), contrastive learning, and semantic inversion—constantly weaving new math formulas into transformer losses and progressive training schedules. On the engineering side I built a real-time AR teaching system on Jetson Nano, and bridged ROS2 with Unity WebSockets to control robotic arms and car models.</p>
  <p>When I’m not coding, you’ll find me at the gym (4–5×/week weightlifting), shooting hoops with Allen Iverson on loop, or whipping up pixel art avatars. I’m also a meme-powered TOEFL prepper—XP points, speed challenges, and creative sentence drills keep me sharp. Looking ahead, I’m plotting a summer U.S. campus tour to connect with labs for my Fall ’27 PhD in general, multimodal AI.</p>
  <p>Above all, I dream of crafting general, multimodal models that bridge vision and language—tools that democratize medical insights, spark creativity, and unite cultures. Thanks for stopping by my pixel portfolio!</p>
</section>